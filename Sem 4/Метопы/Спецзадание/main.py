# Импортируем необходимые функции и библиотеки из библиотеки numpy.
# exp(x) = e^x
# dot - матричное перемножение
# random - генерация случайных чисел
# array - создание массива
from numpy import exp, array, random, dot

# Задаем входные данные для обучающего набора. 
# Каждый элемент - набор значений подаваемый "на вход" нейронов
training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])

# Задаем ожидаемые выходные данные для обучающего набора. 
# Каждый элемент соответствует ожидаемому выходному значению для соответсвенного набора данных выше.
training_set_outputs = array([[0, 1, 1, 0]]).T

# Инициализируем веса случайными значениями в диапазоне [-1, 1]. Матрица размерности (3, 1) для соответствия входам.
synaptic_weights = 2 * random.random((3, 1)) - 1

# Начинаем цикл обучения нейронной сети, который повторяется 10,000 раз.
for iteration in range(10000):

    # Получаем результат для текущих весов
    # output - результат работы нейронной сети
    # output = 1 / 1 + e^(-(w1*x1 + w2*x2 + w3*x3))
    # Где w - вес
    # x - входной нейрон
    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))

    # Вычисляем ошибку обучения, учитывая ожидаемый выход, текущий выход и градиент функции активации.
    # Используем градиентный спуск для корректировки весов.
    # error = (ожидаемый выход - текущий выход) * текущий выход * (1 - текущий выход)
    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))

# После завершения обучения выводим результат предсказания для нового входа [1, 0, 0].
print((1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights))))))


